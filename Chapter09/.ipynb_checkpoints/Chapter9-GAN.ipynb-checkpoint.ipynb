{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GAN framework is comprised of two neural networks, the generator, and discriminator. In the context of image generation, the generator generates fake data, when given noise as input, and the discriminator classifies real images from fake images. During training, the generator and the discriminator compete with each other in a game and as a result, get better at their jobs. The generator tries to generate better-looking images to fool the discriminator and the discriminator tries to get better at identifying real images from fake images.\n",
    "\n",
    "To train a GAN, we need a training dataset. Given a training dataset, the GAN will learn to generate new data with the same distribution as the training dataset. For instance, if we train a GAN on cat images, it will learn to generate new cat images that look real to our eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# path to store/load data\n",
    "path2data=\"./data\"\n",
    "os.makedirs(path2data, exist_ok= True)\n",
    "    \n",
    "\"\"\"\n",
    "The original images might be in different sizes, thus we used a Resize transformation \n",
    "to resize images to 64 by 64. Next, ToTensor scales the image pixels to the range of \n",
    "[0, 1]. Next, we applied a normalization. The normalization mean and std values were set \n",
    "to normalize inputs to the range of [-1, 1]. As you will find out in Defining the \n",
    "Generator and Discriminator recipe, the output of the generator model is a tanh function \n",
    "that generates outputs in the range [-1, 1]\n",
    "\"\"\"\n",
    "h, w = 64, 64\n",
    "mean = (0.5, 0.5, 0.5)\n",
    "std = (0.5, 0.5, 0.5)\n",
    "transform= transforms.Compose([\n",
    "           transforms.Resize((h,w)),\n",
    "           transforms.CenterCrop((h,w)),\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize(mean, std)])\n",
    "    \n",
    "train_ds=datasets.STL10(path2data, split='train', \n",
    "                        download=True,\n",
    "                        transform=transform)\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "As expected, the extracted sample is a PyTorch tensor in the shape of (3, height, width) \n",
    "and is normalized to the range of [-1, 1].\n",
    "\"\"\"\n",
    "for x, _ in train_ds:\n",
    "    print(x.shape, torch.min(x), torch.max(x))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for x,y in train_ds:\n",
    "    print(x.shape,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# since the tensor was normalized to [-1, 1], we had to re-normalize it for visualization purposes\n",
    "plt.imshow(to_pil_image(0.5*x+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32 # chosen\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    nz: The size of the input noise vector (set to 100)\n",
    "    ngf: A coefficient for the number of convolutional filters in the generator (set to 64)\n",
    "    noc: The number of output channels (set to 3 for RGB images)\n",
    "    \n",
    "    A conv-transpose layer is also called a fractionally-strided convolution or a \n",
    "    deconvolution. They are used to upsample the input vector to the desired output \n",
    "    size.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super(Generator, self).__init__()\n",
    "        nz = params[\"nz\"]\n",
    "        ngf = params[\"ngf\"]\n",
    "        noc = params[\"noc\"]\n",
    "        self.dconv1 = nn.ConvTranspose2d( nz, ngf * 8, kernel_size=4,\n",
    "                                         stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(ngf * 8)\n",
    "        self.dconv2 = nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, \n",
    "                                         stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf * 4)\n",
    "        self.dconv3 = nn.ConvTranspose2d( ngf * 4, ngf * 2, kernel_size=4, \n",
    "                                         stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf * 2)\n",
    "        self.dconv4 = nn.ConvTranspose2d( ngf * 2, ngf, kernel_size=4, \n",
    "                                         stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf)\n",
    "        self.dconv5 = nn.ConvTranspose2d( ngf, noc, kernel_size=4, \n",
    "                                         stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.dconv1(x)))\n",
    "        x = F.relu(self.bn2(self.dconv2(x)))            \n",
    "        x = F.relu(self.bn3(self.dconv3(x)))        \n",
    "        x = F.relu(self.bn4(self.dconv4(x)))    \n",
    "        out = torch.tanh(self.dconv5(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gen = {\n",
    "        \"nz\": 100,\n",
    "        \"ngf\": 64,\n",
    "        \"noc\": 3,\n",
    "        }\n",
    "model_gen = Generator(params_gen)\n",
    "device = torch.device(\"cuda:3\")\n",
    "model_gen.to(device)\n",
    "print(model_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that the model was created properly, we passed some dummy input to the generator model. As expected, the model output is a tensor of shape [1, 3, 64, 64]\n",
    "with torch.no_grad():\n",
    "    y= model_gen(torch.zeros(1,100,1,1, device=device))\n",
    "print(y.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similarly, in the __init__ method, we defined the layers and in the forward method, we \n",
    "defined the connections between the layers. Notice that we did not use any pooling layers \n",
    "and instead set the stride argument to 2 or 4 to downsample the input size. Also, notice \n",
    "that leaky_relu activation was used instead of relu to reduce overfitting.\n",
    "\"\"\"\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Discriminator, self).__init__()\n",
    "        nic= params[\"nic\"]\n",
    "        ndf = params[\"ndf\"]\n",
    "        self.conv1 = nn.Conv2d(nic, ndf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 2)            \n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2, True)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2, inplace = True)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2, inplace = True)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2, inplace = True)        \n",
    "        \n",
    "        out = torch.sigmoid(self.conv5(x))\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dis = {\n",
    "    \"nic\": 3,\n",
    "    \"ndf\": 64}\n",
    "model_dis = Discriminator(params_dis)\n",
    "model_dis.to(device)\n",
    "print(model_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample\n",
    "with torch.no_grad():\n",
    "    y= model_dis(torch.zeros(1,3,h,w, device=device))\n",
    "print(y.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The DCGAN paper suggested initializing the weights using a normal distribution with \n",
    "mean=0 and std=0.02, as we did in the helper function.\n",
    "\"\"\"\n",
    "def initialize_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gen.apply(initialize_weights);\n",
    "model_dis.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models to learn, we need to define a criterion. The discriminator model is a classification network and we can use the binary cross-entropy (BCE) loss function as its criterion. For the generator model to learn, we pass its output to the discriminator model and then evaluate the output of the discriminator model. Thus, the same BCE loss function can be used as a criterion to train the generator model. Also, we will use the Adam optimizer to update the parameters of the discriminator and generator models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\"\"\"\n",
    "we defined the Adam optimizer from torch.optim for the generator model based on the \n",
    "hyperparameters suggested in the DCGAN paper. The paper suggested setting the learning \n",
    "rate to 0.0002 and the momentum term beta1 to 0.5 for training stability.\n",
    "\"\"\"\n",
    "lr = 2e-4 \n",
    "beta1 = 0.5\n",
    "opt_dis = optim.Adam(model_dis.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(model_gen.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the GAN framework is done in two stages: training the discriminator and training the generator. To this end, we will take the following steps:\n",
    "\n",
    "1. Get a batch of real images with the target labels set to 1.\n",
    "2. Generate a batch of fake images using the generator with the target labels set to 0.\n",
    "3. Feed the mini-batches to the discriminator and compute the loss and gradients.\n",
    "4. Update the discriminator parameters using the gradients.\n",
    "5. Generate a batch of fake images using the generator with the target labels set to 1.\n",
    "6. Feed the fake mini-batch to the discriminator and compute the loss and gradients.\n",
    "7. Update the generator only based on gradients.\n",
    "8. Repeat from step 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we defined a few parameters. We defined real_label and fake_label and set them to 1 and 0, \n",
    "respectively. Later, we will need to label a mini-batch using these parameters. The nz \n",
    "parameter specifies the size of the input noise vector to the generator model. This was \n",
    "set to 100 in Defining the generator and discriminator recipe. The num_epochs parameter \n",
    "specifies how many times we want to iterate over the training data. To store the loss \n",
    "values for the discriminator and generator models, we defined the loss_history dictionary.\n",
    "\"\"\"\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "nz = params_gen[\"nz\"]\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "loss_history={\"gen\": [],\n",
    "              \"dis\": []}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "we implemented the training loop. The training loop iterates over the real dataset \n",
    "for num_epochs. In each epoch, we got a batch of real images from celeb_dl and fed it \n",
    "to the discriminator model and got its output as out_dis. Note that here, the real \n",
    "images were labeled with real_label using the torch.full method. Then, the loss value \n",
    "for the real mini-batch was calculated as loss_r. Next, the gradients of loss_r with \n",
    "respect to the discriminator parameters were calculated in a backward pass.\n",
    "\"\"\"\n",
    "batch_count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        ba_si = xb.size(0)\n",
    "        model_dis.zero_grad()\n",
    "        xb = xb.to(device)\n",
    "        yb = torch.full((ba_si,), real_label, device=device)\n",
    "        out_dis = model_dis(xb)\n",
    "        loss_r = loss_func(out_dis, yb)\n",
    "        loss_r.backward()\n",
    "\n",
    "        \"\"\"\n",
    "        In passing the output of the generator to the discriminator, we used the .detach() \n",
    "        method to avoid gradient tracking for the generator model. Note that at this point, \n",
    "        the fake images were labeled with fake_label using the torch.fill_ method. Then, \n",
    "        the loss value for the fake mini-batch was calculated as loss_f. Next, the gradients \n",
    "        of loss_f with respect to the discriminator parameters were calculated in a backward \n",
    "        pass.\n",
    "        \"\"\"\n",
    "        noise = torch.randn(ba_si, nz, 1, 1, device=device)\n",
    "        out_gen = model_gen(noise)\n",
    "        out_dis = model_dis(out_gen.detach())\n",
    "        yb.fill_(fake_label)    \n",
    "        loss_f = loss_func(out_dis, yb)\n",
    "        loss_f.backward()\n",
    "        loss_dis = loss_r + loss_f  \n",
    "        opt_dis.step()   \n",
    "\n",
    "        \"\"\"\n",
    "        Next, we trained the generator model. To this end, we passed the fake images to \n",
    "        the discriminator model and got its output. Note that here, the fakes images were \n",
    "        labeled with real_label using the .fill_ method. This may sound strange at first, \n",
    "        but it is done to force the generator model to generate better-looking images.\n",
    "        \"\"\"\n",
    "        model_gen.zero_grad()\n",
    "        yb.fill_(real_label)  \n",
    "        out_dis = model_dis(out_gen)\n",
    "        loss_gen = loss_func(out_dis, yb)\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        loss_history[\"gen\"].append(loss_gen.item())\n",
    "        loss_history[\"dis\"].append(loss_dis.item())\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:\n",
    "            print(epoch, loss_gen.item(),loss_dis.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Loss Progress\")\n",
    "plt.plot(loss_history[\"gen\"],label=\"Gen. Loss\")\n",
    "plt.plot(loss_history[\"dis\"],label=\"Dis. Loss\")\n",
    "plt.xlabel(\"batch count\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store models\n",
    "import os\n",
    "path2models = \"./models/\"\n",
    "os.makedirs(path2models, exist_ok=True)\n",
    "path2weights_gen = os.path.join(path2models, \"weights_gen_128.pt\")\n",
    "path2weights_dis = os.path.join(path2models, \"weights_dis_128.pt\")\n",
    "\n",
    "torch.save(model_gen.state_dict(), path2weights_gen)\n",
    "torch.save(model_dis.state_dict(), path2weights_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've trained a GAN, we end up with two trained models. Usually, we discard the discriminator model and keep the generator model. We can use the trained generator to generate new images. To deploy the generator model, we load the trained weights into the model and then feed it with random noise. Make sure to define the model class beforehand. To avoid repetition, we will not define the model class here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights:\n",
    "weights = torch.load(path2weights_gen)\n",
    "model_gen.load_state_dict(weights)\n",
    "model_gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# set the model in evaluation mode\n",
    "with torch.no_grad():\n",
    "    # fed random noise vectors into the model and received generated fake images\n",
    "    fixed_noise = torch.randn(16, nz, 1, 1, device=device)\n",
    "    print(fixed_noise.shape)\n",
    "    img_fake = model_gen(fixed_noise).detach().cpu()    \n",
    "print(img_fake.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for ii in range(16):\n",
    "    plt.subplot(4,4,ii+1)\n",
    "    plt.imshow(to_pil_image(0.5*img_fake[ii]+0.5)) # re-normalize the output tensor back to its original values for visualization purposes\n",
    "    plt.axis(\"off\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the generated images. Some of them may look very distorted, while others look relatively realistic. To improve the results, you can train the model on a single class of data as opposed to multiple classes together. GANs perform better when they are trained with a single class. The STL-10 dataset has multiple classes. Try to select one category and the train the GAN models. Also, you can try to train the model for a longer time and see how that changes the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
