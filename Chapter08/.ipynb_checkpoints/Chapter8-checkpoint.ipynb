{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural style transfer, we take a content image and a style image. Then, we generate an image to have the content of the content image and the artistic style of the style image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "path2content= \"./data/content.jpg\"\n",
    "path2style= \"./data/style.jpg\"\n",
    "content_img = Image.open(path2content)\n",
    "style_img = Image.open(path2style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "h, w = 256, 384 \n",
    "mean_rgb = (0.485, 0.456, 0.406)\n",
    "std_rgb = (0.229, 0.224, 0.225)\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "                    transforms.Resize((h,w)),  \n",
    "                    transforms.ToTensor(), # scale the pixels' values to the range of [0, 1]\n",
    "                    transforms.Normalize(mean_rgb, std_rgb)])  # depend on the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tensor = transformer(content_img)\n",
    "print(content_tensor.shape, content_tensor.requires_grad) # no optimization is applied to the content and style images, this attribute should be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_tensor = transformer(style_img)\n",
    "print(style_tensor.shape, style_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = content_tensor.clone().requires_grad_(True)\n",
    "print(input_tensor.shape, input_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def imgtensor2pil(img_tensor):\n",
    "    img_tensor_c = img_tensor.clone().detach() # cloned the tensor to prevent making any changes to the original tensor\n",
    "    \n",
    "    # normalize back from zero-mean-unit-variance normalization to original values\n",
    "    img_tensor_c*=torch.tensor(std_rgb).view(3, 1,1)\n",
    "    img_tensor_c+=torch.tensor(mean_rgb).view(3, 1,1)\n",
    "    \n",
    "    img_tensor_c = img_tensor_c.clamp(0,1) # made sure that the values are in the range [0, 1] by using the .clamp()\n",
    "    img_pil=to_pil_image(img_tensor_c)\n",
    "    return img_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen throughout this book, we have followed a few standard steps: we loaded the input and target data, defined a model, an objective function and optimizer, and then trained the model by updating the model parameters using the gradient-descent algorithm. In all of these past cases, the input to the model was kept unchanged during the training process and the model was updated.\n",
    "\n",
    "Now, imagine a situation whereby we keep the model parameters fixed and instead update the input to the model during training. This twist is the intuition behind the neural style transfer algorithm.\n",
    "\n",
    "Specifically, the neural style transfer algorithm works as follows:\n",
    "\n",
    "1. Take a pretrained classification model (for example, VGG19), remove the last layers, and keep the remaining layers to serve as a feature extractor.\n",
    "\n",
    "2. Feed the content image to the model and get selected features to serve as the target content.\n",
    "\n",
    "3. Feed the style image to the model and get the Gram matrix of selected features to serve as the target style.\n",
    "\n",
    "4. Feed the input to the model and get the features and the Gram matrix of selected features to serve as the predicted content and style, respectively.\n",
    "\n",
    "5. Compute the content and style errors, and use this information to update the input and reduce the error.\n",
    "\n",
    "6. Repeat step 4 until the error is minimized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "plt.imshow(imgtensor2pil(content_tensor))\n",
    "plt.title(\"content image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgtensor2pil(style_tensor))\n",
    "plt.title(\"style image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "The model expects the input to be mini-batches of shape (3, Height, Width) and normalized. \n",
    "This is why we transformed the content and style images in the Loading the data section.\n",
    "\"\"\"\n",
    "model_vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Freeze the model parameters\n",
    "\"\"\"\n",
    "We froze the model parameters using the requires_grad_ method to avoid any changes to the \n",
    "model during the algorithm optimization. \n",
    "???\n",
    "\"\"\"\n",
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad_(False)   \n",
    "print(model_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the intermediate features of the pretrained model. These features will be used in \n",
    "calculating the style and content loss values.\n",
    "\"\"\"\n",
    "def get_features(x, model, layers):\n",
    "    features = {}\n",
    "    for name, layer in enumerate(model.children()):\n",
    "        x = layer(x)\n",
    "        if str(name) in layers:\n",
    "            features[layers[str(name)]] = x\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the Gram matrix of a tensor, which will be used to calculate the style loss value.\n",
    "The input tensor x comes from the intermediate features of the model. In the helper function,\n",
    "we reshaped x from a 4D tensor to a 2D tensor and then calculated the Gram matrix. The \n",
    "output is a tensor of shape [c, c].\n",
    "\"\"\"\n",
    "def gram_matrix(x):\n",
    "    # x: A tensor of shape [1, c, h, w], where c, h, w are the number of channels, height, and width of x\n",
    "    n, c, h, w = x.size() \n",
    "    x = x.view(n*c, h * w) \n",
    "    gram = torch.mm(x, x.t()) \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\"\"\"\n",
    "Compute the content loss. We extracted the target and predicted tensors for the layer \n",
    "specified by the argument layer. Then, we calculated the mean squared error (MSE) between \n",
    "the two tensors and returned its value.\n",
    "\"\"\"\n",
    "def get_content_loss(pred_features, target_features, layer):\n",
    "    \"\"\"\n",
    "    pred_features: A Python dictionary containing the intermediate features of the model given the input tensor\n",
    "    target_features: A Python dictionary containing the intermediate features of the model given the content tensor\n",
    "    layer: A string containing the layer name\n",
    "    \"\"\"\n",
    "    target= target_features[layer]\n",
    "    pred = pred_features [layer]\n",
    "    loss = F.mse_loss(pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We iterated over the style layers and extracted the predicted and target tensors per layer. \n",
    "Then, we calculated the Gram matrix for the two tensors and used them to compute the MSE. \n",
    "The loss value was calculated per layer, multiplied by the layer weight, normalized and then \n",
    "added all together. The function returned the accumulated loss value for all the layers \n",
    "included in the style loss.\n",
    "\"\"\"\n",
    "def get_style_loss(pred_features, target_features, style_layers_dict):  \n",
    "    \"\"\"\n",
    "    pred_features: A Python dictionary containing intermediate features of the model given the input tensor\n",
    "    target_features: A Python dictionary containing intermediate features of the model given the style tensor\n",
    "    style_layers_dict: A Python dictionary containing the name and weight of the layers included in the style loss\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for layer in style_layers_dict:\n",
    "        pred_fea = pred_features[layer]\n",
    "        pred_gram = gram_matrix(pred_fea)\n",
    "        n, c, h, w = pred_fea.shape\n",
    "        target_gram = gram_matrix (target_features[layer])\n",
    "        layer_loss = style_layers_dict[layer] *  F.mse_loss(pred_gram, target_gram)\n",
    "        loss += layer_loss/ (n* c * h * w)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features for the content and style images\n",
    "\"\"\"\n",
    "We called the get_features helper function to get the content and style features. \n",
    "To get the features, we passed the content tensor and style tensor to the helper function. \n",
    "Notice that we added a dimension to the tensors using the .unsqueeze method since the model \n",
    "input shape is [1, 3, height, width]. The name and number of the layers were defined in the \n",
    "Python dictionary, feature_layers. \n",
    "\"\"\"\n",
    "feature_layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1',\n",
    "                  '10': 'conv3_1',\n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  \n",
    "                  '28': 'conv5_1'}\n",
    "\n",
    "con_tensor = content_tensor.unsqueeze(0).to(device)\n",
    "sty_tensor = style_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "content_features = get_features(con_tensor, model_vgg, feature_layers)\n",
    "style_features = get_features(sty_tensor, model_vgg, feature_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debugging purposes only\n",
    "for key in content_features.keys():\n",
    "    print(content_features[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\"\"\"\n",
    "We defined the input tensor. If you recall, the goal in the neural style transfer \n",
    "algorithm is to update the input to minimize the loss function. The input can be \n",
    "initialized randomly or with the content image. As was observed, we cloned the content \n",
    "tensor as the input tensor. Notice that the requires_grad method should be set to True \n",
    "since we want to be able to update the input tensor.\n",
    "\"\"\"\n",
    "input_tensor = con_tensor.clone().requires_grad_(True) # Initialize the input tensor with the content tensor:\n",
    "optimizer = optim.Adam([input_tensor], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameters\n",
    "\"\"\"\n",
    "These parameters define the contributions of the content loss and style loss in the \n",
    "overall loss value. A higher style_weight parameter is usually desirable compared to \n",
    "content_weight, but you can play around with the values to see their contributions.\n",
    "\"\"\"\n",
    "num_epochs = 300\n",
    "content_weight = 1e1\n",
    "style_weight = 1e4\n",
    "content_layer = \"conv5_1\" # we used conv5_1 as the content layer. You can set this parameter to a different layer and see the impact on the outcome\n",
    "# define the name and weight of the layers included in the style loss, can also be changed as you desire\n",
    "style_layers_dict = { 'conv1_1': 0.75,\n",
    "                      'conv2_1': 0.5,\n",
    "                      'conv3_1': 0.25,\n",
    "                      'conv4_1': 0.25,\n",
    "                      'conv5_1': 0.25}\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    optimizer.zero_grad()\n",
    "    input_features = get_features(input_tensor, model_vgg, feature_layers)\n",
    "    content_loss = get_content_loss (input_features, content_features, content_layer)\n",
    "    style_loss = get_style_loss(input_features, style_features, style_layers_dict)\n",
    "    neural_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    neural_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}, content loss: {:.2}, style loss {:.2}'.format(\n",
    "          epoch,content_loss, style_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgtensor2pil(input_tensor[0].cpu()));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
